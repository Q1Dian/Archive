{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2132ab33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /home/jefferyfan/ainotebook/.conda/lib/python3.11/site-packages (0.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9fa1d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import vit_b_16\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1d22a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lambda_rep = 1.0  # Weight for JEPA predictive loss\n",
    "lambda_flow = 0.5  # Weight for optical flow loss\n",
    "lambda_smooth = 0.1  # Weight for flow smoothness loss\n",
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-4\n",
    "max_displacement = 4  # For correlation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18222d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard setup\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = f\"runs/jepa_flow_hybrid_{timestamp}\"\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a654c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for video frames (dummy implementation)\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, frame_size=224):\n",
    "        self.num_samples = num_samples\n",
    "        self.frame_size = frame_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate random adjacent frames with some motion\n",
    "        x_t = torch.rand(3, self.frame_size, self.frame_size)\n",
    "\n",
    "        # Create x_t1 by applying a simple transformation to x_t\n",
    "        # In a real dataset, this would be actual video frames\n",
    "        affine_mat = torch.eye(2, 3)\n",
    "        affine_mat[0, 2] = torch.rand(1) * 0.1  # random x translation\n",
    "        affine_mat[1, 2] = torch.rand(1) * 0.1  # random y translation\n",
    "\n",
    "        grid = F.affine_grid(affine_mat.unsqueeze(0), x_t.unsqueeze(0).shape)\n",
    "        x_t1 = F.grid_sample(x_t.unsqueeze(0), grid)\n",
    "        x_t1 = x_t1.squeeze(0)\n",
    "\n",
    "        return x_t, x_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "740aa4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Layer for optical flow\n",
    "class CorrelationLayer(nn.Module):\n",
    "    def __init__(self, max_displacement=4):\n",
    "        super().__init__()\n",
    "        self.max_displacement = max_displacement\n",
    "        self.kernel_size = 2 * max_displacement + 1\n",
    "\n",
    "    def forward(self, f_t, f_t1):\n",
    "        b, c, h, w = f_t.size()\n",
    "        corr = torch.zeros(b, self.kernel_size * self.kernel_size, h, w).to(f_t.device)\n",
    "\n",
    "        # Pad feature maps for displacement\n",
    "        padding = self.max_displacement\n",
    "        f_t1_padded = F.pad(f_t1, (padding, padding, padding, padding))\n",
    "\n",
    "        # Compute correlation for each displacement\n",
    "        for i in range(-self.max_displacement, self.max_displacement + 1):\n",
    "            for j in range(-self.max_displacement, self.max_displacement + 1):\n",
    "                # Extract shifted version of f_t1\n",
    "                shifted_f_t1 = f_t1_padded[\n",
    "                    :, :, padding + i : padding + i + h, padding + j : padding + j + w\n",
    "                ]\n",
    "\n",
    "                # Compute correlation\n",
    "                idx = (i + self.max_displacement) * self.kernel_size + (\n",
    "                    j + self.max_displacement\n",
    "                )\n",
    "                corr[:, idx, :, :] = (f_t * shifted_f_t1).sum(dim=1)\n",
    "\n",
    "        return corr\n",
    "\n",
    "\n",
    "# Flow Head with correlation\n",
    "class FlowHead(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, hidden_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels // 2, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            hidden_channels // 2, 2, 3, padding=1\n",
    "        )  # Output flow field (u, v)\n",
    "\n",
    "    def forward(self, corr_volume):\n",
    "        x = F.relu(self.conv1(corr_volume))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        flow = self.conv3(x)\n",
    "        return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c4d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial JEPA Predictor\n",
    "class SpatialPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_dim, hidden_dim, 1)  # 1x1 convolution\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, output_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1cc4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMA Teacher Network\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "        # Register shadow parameters\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                new_average = (\n",
    "                    1.0 - self.decay\n",
    "                ) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3939e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiable warping function\n",
    "def warp(x, flow):\n",
    "    # x: [B, C, H, W]\n",
    "    # flow: [B, 2, H, W] (dx, dy)\n",
    "    B, C, H, W = x.size()\n",
    "\n",
    "    # Create grid\n",
    "    grid_y, grid_x = torch.meshgrid(torch.arange(0, H), torch.arange(0, W))\n",
    "    grid = torch.stack((grid_x, grid_y), 2).float().to(x.device)\n",
    "    grid = grid.unsqueeze(0).repeat(B, 1, 1, 1)  # [B, H, W, 2]\n",
    "\n",
    "    # Add flow to grid\n",
    "    new_grid = grid + flow.permute(0, 2, 3, 1)\n",
    "\n",
    "    # Normalize grid to [-1, 1]\n",
    "    new_grid[:, :, :, 0] = 2.0 * new_grid[:, :, :, 0] / max(W - 1, 1) - 1.0\n",
    "    new_grid[:, :, :, 1] = 2.0 * new_grid[:, :, :, 1] / max(H - 1, 1) - 1.0\n",
    "\n",
    "    # Sample using grid_sample\n",
    "    warped = F.grid_sample(x, new_grid, align_corners=True)\n",
    "    return warped\n",
    "\n",
    "\n",
    "# Smoothness loss for optical flow\n",
    "def smoothness_loss(flow):\n",
    "    # Calculate gradients of flow\n",
    "    dx = torch.abs(flow[:, :, :, :-1] - flow[:, :, :, 1:])\n",
    "    dy = torch.abs(flow[:, :, :-1, :] - flow[:, :, 1:, :])\n",
    "\n",
    "    # Sum of all gradients\n",
    "    loss = dx.sum() + dy.sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d3d2717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main model\n",
    "class JEPAFlowHybrid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Backbone - ViT with spatial output\n",
    "        self.backbone = vit_b_16(pretrained=True)\n",
    "\n",
    "        # Modify ViT to output spatial features\n",
    "        self.backbone.heads = nn.Identity()  # Remove classification head\n",
    "\n",
    "        # JEPA predictor\n",
    "        self.jepa_predictor = SpatialPredictor(768, 768)\n",
    "\n",
    "        # Correlation layer\n",
    "        self.correlation = CorrelationLayer(max_displacement=max_displacement)\n",
    "\n",
    "        # Flow head\n",
    "        self.flow_head = FlowHead(max_displacement * 2 + 1)\n",
    "\n",
    "        # EMA teacher\n",
    "        self.teacher = EMA(self.backbone)\n",
    "\n",
    "    def forward(self, x_t, x_t1):\n",
    "        # Extract features\n",
    "        f_t = self.backbone(x_t)  # [B, 768, H/16, W/16] for ViT-B/16\n",
    "        f_t1 = self.backbone(x_t1)\n",
    "\n",
    "        # JEPA prediction\n",
    "        f_t1_pred = self.jepa_predictor(f_t)\n",
    "\n",
    "        # Flow estimation\n",
    "        corr_volume = self.correlation(f_t, f_t1)\n",
    "        flow = self.flow_head(corr_volume)\n",
    "\n",
    "        return f_t1_pred, flow, f_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d19c341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jefferyfan/ainotebook/.conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jefferyfan/ainotebook/.conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /home/jefferyfan/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330M/330M [00:34<00:00, 10.1MB/s] \n",
      "/home/jefferyfan/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/functional.py:5082: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/jefferyfan/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/functional.py:5015: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [8, 768]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m x_t1 = x_t1.to(device)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m f_t1_pred, flow, f_t1 = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# JEPA loss\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mJEPAFlowHybrid.forward\u001b[39m\u001b[34m(self, x_t, x_t1)\u001b[39m\n\u001b[32m     27\u001b[39m f_t1 = \u001b[38;5;28mself\u001b[39m.backbone(x_t1)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# JEPA prediction\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m f_t1_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjepa_predictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Flow estimation\u001b[39;00m\n\u001b[32m     33\u001b[39m corr_volume = \u001b[38;5;28mself\u001b[39m.correlation(f_t, f_t1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mSpatialPredictor.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     x = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     10\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.conv2(x)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [8, 768]"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = JEPAFlowHybrid().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = VideoFrameDataset(num_samples=1000)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_rep_loss = 0.0\n",
    "    epoch_flow_loss = 0.0\n",
    "    epoch_total_loss = 0.0\n",
    "\n",
    "    for batch_idx, (x_t, x_t1) in enumerate(dataloader):\n",
    "        # Move data to device\n",
    "        x_t = x_t.to(device)\n",
    "        x_t1 = x_t1.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        f_t1_pred, flow, f_t1 = model(x_t, x_t1)\n",
    "\n",
    "        # JEPA loss\n",
    "        with torch.no_grad():\n",
    "            f_t1_target = model.teacher.model(x_t1)\n",
    "        rep_loss = F.mse_loss(f_t1_pred, f_t1_target)\n",
    "\n",
    "        # Flow loss\n",
    "        x_t1_warped = warp(x_t, flow)\n",
    "        photo_loss = F.l1_loss(x_t1_warped, x_t1)\n",
    "        smooth_loss = smoothness_loss(flow)\n",
    "        flow_loss = photo_loss + lambda_smooth * smooth_loss\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = lambda_rep * rep_loss + lambda_flow * flow_loss\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update EMA teacher\n",
    "        model.teacher.update()\n",
    "\n",
    "        # Log losses\n",
    "        writer.add_scalar(\"Loss/JEPA\", rep_loss.item(), global_step)\n",
    "        writer.add_scalar(\"Loss/Flow\", flow_loss.item(), global_step)\n",
    "        writer.add_scalar(\"Loss/Total\", total_loss.item(), global_step)\n",
    "        writer.add_scalar(\"Loss/Photometric\", photo_loss.item(), global_step)\n",
    "        writer.add_scalar(\"Loss/Smoothness\", smooth_loss.item(), global_step)\n",
    "\n",
    "        # Update epoch statistics\n",
    "        epoch_rep_loss += rep_loss.item()\n",
    "        epoch_flow_loss += flow_loss.item()\n",
    "        epoch_total_loss += total_loss.item()\n",
    "        global_step += 1\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(dataloader)}], \"\n",
    "                f\"JEPA Loss: {rep_loss.item():.4f}, Flow Loss: {flow_loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    # Log epoch averages\n",
    "    avg_rep_loss = epoch_rep_loss / len(dataloader)\n",
    "    avg_flow_loss = epoch_flow_loss / len(dataloader)\n",
    "    avg_total_loss = epoch_total_loss / len(dataloader)\n",
    "\n",
    "    writer.add_scalar(\"Epoch/JEPA\", avg_rep_loss, epoch)\n",
    "    writer.add_scalar(\"Epoch/Flow\", avg_flow_loss, epoch)\n",
    "    writer.add_scalar(\"Epoch/Total\", avg_total_loss, epoch)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "        f\"Avg JEPA Loss: {avg_rep_loss:.4f}, Avg Flow Loss: {avg_flow_loss:.4f}, \"\n",
    "        f\"Avg Total Loss: {avg_total_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()\n",
    "print(\"Training completed. Run 'tensorboard --logdir=runs' to view the loss plots.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
