# Knowledge Distillation

## Motif

Knowledge distillation has emerged as a widely used technique in machine learning, particularly for compressing large models into smaller, more efficient ones. It piqued my interest when I began reading about machine learning research, but lately, I've grown more skeptical. After exploring resources like [this notebook](https://github.com/geohot/ai-notebooks/blob/master/cifar10_distillation.ipynb), which provides mixed results, and papers such as [Mamba in the Llama](https://arxiv.org/pdf/2408.15237) which stood by its effectiveness, I began to wonder: Is knowledge distillation really as effective as its proponents claim?

In this project, I’ll share my thoughts and questions about distillation, trying to understand where it shines and where it might fall short.

## What is Knowledge Distillation?

At its core, knowledge distillation is a model compression technique. The basic idea is to take a large, accurate “teacher” model and transfer its learned knowledge to a smaller “student” model. The goal is for the student to replicate the teacher’s performance, or come close to it, while using significantly fewer resources (parameters, memory, etc.) by having higher quality\* synthesized data, assuming the teacher model retains good insight on the transferred representation (attentionmap, softmax distribution, relational information, etc.), for training at a certain arbitrary layer.

Additionally, models like **DistilBERT** and **Mamba in the Llama** initialize the student’s weights from the teacher, potentially speeding up convergence.

## Key Questions I have About Knowledge Distillation

#### 1. Is distillation better than training directly on the original dataset?
In my unverified opinion, knowledge distillation caps out the upper bound of the student model because. I wonder if, given a prediction task, only the logits of the correct predictions (by the teacher model) will be used, while the incorrect predictions will be trained normally in the hopes that it will learn the same thing differently (proof by exhaustion), will result in a performance increase?

#### 2. What parts of the model benefit most from distillation?

#### 3. What can be pruned or simplified during distillation?

## Instincts on Distillation’s Future

From what I’ve observed, knowledge distillation works best in specific contexts, such as compressing extremely large models or adapting models for real-time deployment. However, applying it to every part of the model might be overkill. The challenge lies in figuring out which components of a neural network truly need to be distilled and which can be simplified or discarded without losing essential information.

**Dropout** and other regularization techniques suggest that many neural networks are overparameterized. This raises the question: Can we selectively distill important channels or pathways while trimming down the rest, instead of distilling the entire model? The key to effective distillation might lie in finding the optimal balance between compression and preservation.

## Research

Experiment 1:

Models in https://huggingface.co/Hifo/KDExperiment/tree/main
| Teacher Model | Teacher Accuracy | Distil Model | Distil Accuracy | Raw Student Accuracy |
| ------------: | ---------------: | -----------: | --------------: | -------------------: |
| ResNet-50 | 84.70% | ResNet-18 | 77.41% | 76.48% |



## Conclusion




## References

- [srush talk about mamba in the llama](https://youtu.be/A5ff8hu1amM) and [its paper](https://arxiv.org/pdf/2408.15237)
- [pytorch KD tutorial](https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html)
- [geohot notebook](https://github.com/geohot/ai-notebooks/blob/master/cifar10_distillation.ipynb)
- 
