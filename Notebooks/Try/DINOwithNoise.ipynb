{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10341b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "import os\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2c56df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4afbd4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af48d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT_S_16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            \"vit_small_patch16_224\", pretrained=False, num_classes=0, global_pool=\"\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da97b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DINO like structure with noise instead of masking for x1 and x2, the goal is to have the representation of the clean iamge to be the same as itself with additional guassian noise added on it\n",
    "def gather(consts: torch.Tensor, t: torch.Tensor):\n",
    "    \"\"\"Gather constants for specific timesteps t.\"\"\"\n",
    "    c = consts.gather(-1, t)\n",
    "    return c.reshape(-1, 1, 1, 1)\n",
    "\n",
    "\n",
    "class DINODiffusionTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        student: nn.Module,\n",
    "        teacher: nn.Module,\n",
    "        predictor: nn.Module,\n",
    "        device: torch.device,\n",
    "        n_steps=1000,\n",
    "        beta_start=1e-4,\n",
    "        beta_end=0.02,\n",
    "        momentum=0.996,\n",
    "    ):\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "        self.predictor = predictor\n",
    "        self.device = device\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # Initialize teacher with student weights\n",
    "        self.teacher.load_state_dict(self.student.state_dict())\n",
    "        # Freeze teacher\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Diffusion parameters\n",
    "        self.n_steps = n_steps\n",
    "        self.betas = torch.linspace(beta_start, beta_end, n_steps).to(device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    def update_teacher(self):\n",
    "        \"\"\"EMA update for teacher network\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for param_s, param_t in zip(\n",
    "                self.student.parameters(), self.teacher.parameters()\n",
    "            ):\n",
    "                param_t.data.mul_(self.momentum).add_(\n",
    "                    param_s.data * (1 - self.momentum)\n",
    "                )\n",
    "\n",
    "    def forward_diffusion(self, x_0: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Sample from q(x_t | x_0)\"\"\"\n",
    "        sqrt_alpha_bar = gather(torch.sqrt(self.alpha_bars), t)\n",
    "        sqrt_one_minus_alpha_bar = gather(torch.sqrt(1.0 - self.alpha_bars), t)\n",
    "\n",
    "        noise = torch.randn_like(x_0)\n",
    "        return sqrt_alpha_bar * x_0 + sqrt_one_minus_alpha_bar * noise, noise\n",
    "\n",
    "    def loss(self, x_0: torch.Tensor):\n",
    "        \"\"\"DINO-Diffusion loss function\"\"\"\n",
    "        # Sample random timesteps\n",
    "        t = torch.randint(0, self.n_steps, (x_0.shape[0],), device=self.device)\n",
    "\n",
    "        # Create noisy versions\n",
    "        x_t, noise = self.forward_diffusion(x_0, t)\n",
    "\n",
    "        # Get representations\n",
    "        with torch.no_grad():\n",
    "            # Teacher processes clean image\n",
    "            teacher_repr = self.teacher(x_0)\n",
    "            # Use CLS token as global representation\n",
    "            teacher_global = teacher_repr[:, 0]  # [B, D]\n",
    "            # Center and sharpen teacher output (DINO-specific)\n",
    "            teacher_global = F.softmax(teacher_global / 0.07, dim=-1)\n",
    "\n",
    "        # Student processes noisy image\n",
    "        student_repr = self.student(x_t)\n",
    "        student_global = student_repr[:, 0]  # [B, D]\n",
    "        # Predictor transforms student output\n",
    "        predicted_repr = self.predictor(student_global)\n",
    "\n",
    "        # DINO loss: cross-entropy between predicted and teacher distributions\n",
    "        loss = F.cross_entropy(predicted_repr, teacher_global)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fcd7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(im, path):\n",
    "    Image.fromarray(im).save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e79a2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Student params: 21665664\n",
      "Teacher params: 21665664\n",
      "Predictor params: 1181568\n",
      "Epoch: 0, Step: 0, Loss: 6.009171009063721\n",
      "Step 0: Cosine Sim = 0.4622\n",
      "Epoch: 0, Step: 100, Loss: 1.7871376276016235\n",
      "Epoch: 0, Step: 200, Loss: 1.4661836624145508\n",
      "Step 200: Cosine Sim = 0.7604\n",
      "Epoch: 0, Step: 300, Loss: 1.42730712890625\n",
      "Epoch: 1, Step: 400, Loss: 1.348954200744629\n",
      "Step 400: Cosine Sim = 0.7491\n",
      "Epoch: 1, Step: 500, Loss: 1.581053376197815\n",
      "Epoch: 1, Step: 600, Loss: 1.467311143875122\n",
      "Step 600: Cosine Sim = 0.7685\n",
      "Epoch: 1, Step: 700, Loss: 1.4322378635406494\n",
      "Epoch: 2, Step: 800, Loss: 1.412899136543274\n",
      "Step 800: Cosine Sim = 0.7705\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     95\u001b[39m images = images.to(device)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Forward pass and loss calculation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m loss = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Normalize loss for gradient accumulation\u001b[39;00m\n\u001b[32m    101\u001b[39m loss = loss / config[\u001b[33m\"\u001b[39m\u001b[33maccumulation_steps\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mDINODiffusionTrainer.loss\u001b[39m\u001b[34m(self, x_0)\u001b[39m\n\u001b[32m     71\u001b[39m     teacher_global = F.softmax(teacher_global / \u001b[32m0.07\u001b[39m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Student processes noisy image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m student_repr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m student_global = student_repr[:, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# [B, D]\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Predictor transforms student output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mVIT_S_16.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/timm/models/vision_transformer.py:993\u001b[39m, in \u001b[36mVisionTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m993\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    994\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.forward_head(x)\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/timm/models/vision_transformer.py:948\u001b[39m, in \u001b[36mVisionTransformer.forward_features\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m    946\u001b[39m     x = checkpoint_seq(\u001b[38;5;28mself\u001b[39m.blocks, x)\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    950\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm(x)\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/timm/models/vision_transformer.py:176\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.drop_path1(\u001b[38;5;28mself\u001b[39m.ls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    177\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.drop_path2(\u001b[38;5;28mself\u001b[39m.ls2(\u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.norm2(x))))\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/timm/layers/attention.py:75\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m     70\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     71\u001b[39m         x: torch.Tensor,\n\u001b[32m     72\u001b[39m         attn_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     73\u001b[39m ) -> torch.Tensor:\n\u001b[32m     74\u001b[39m     B, N, C = x.shape\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     qkv = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.reshape(B, N, \u001b[32m3\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_heads, \u001b[38;5;28mself\u001b[39m.head_dim).permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m)\n\u001b[32m     76\u001b[39m     q, k, v = qkv.unbind(\u001b[32m0\u001b[39m)\n\u001b[32m     77\u001b[39m     q, k = \u001b[38;5;28mself\u001b[39m.q_norm(q), \u001b[38;5;28mself\u001b[39m.k_norm(k)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ainotebook/.conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "    # Main\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    os.makedirs(\"samples\", exist_ok=True)\n",
    "\n",
    "    # Configuration\n",
    "    config = {\n",
    "        \"n_steps\": 1000,\n",
    "        \"student_momentum\": 0.996,\n",
    "        \"batch_size\": 128,  # Increased from 1 for better training\n",
    "        \"epochs\": 100,  # Increased for meaningful training\n",
    "        \"save_interval\": 1000,  # Save every 1000 steps\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"accumulation_steps\": 1,  # Gradient accumulation steps\n",
    "        \"image_size\": 224,  # ViT-S/16 expects 224x224 images\n",
    "    }\n",
    "\n",
    "    # Device setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize models\n",
    "    student = VIT_S_16().to(device)\n",
    "    teacher = VIT_S_16().to(device)\n",
    "    embed_dim = student.backbone.num_features\n",
    "    predictor = nn.Sequential(\n",
    "        nn.Linear(embed_dim, 4 * embed_dim),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(4 * embed_dim, embed_dim),\n",
    "    ).to(device)\n",
    "\n",
    "    # Print parameter count\n",
    "    print(f\"Student params: {sum(p.numel() for p in student.parameters())}\")\n",
    "    print(f\"Teacher params: {sum(p.numel() for p in teacher.parameters())}\")\n",
    "    print(f\"Predictor params: {sum(p.numel() for p in predictor.parameters())}\")\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = DINODiffusionTrainer(\n",
    "        student=student,\n",
    "        teacher=teacher,\n",
    "        predictor=predictor,\n",
    "        device=device,\n",
    "        n_steps=config[\"n_steps\"],\n",
    "    )\n",
    "\n",
    "    # Setup optimizer (only optimize student and predictor)\n",
    "    optimizer = optim.AdamW(\n",
    "        list(student.parameters()) + list(predictor.parameters()),\n",
    "        lr=config[\"learning_rate\"],\n",
    "        weight_decay=0.05,\n",
    "    )\n",
    "\n",
    "    # Setup dataset and dataloader\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Recommended datasets for training:\n",
    "    # 1. ImageNet (large-scale, best results)\n",
    "    # 2. Places365 (scene understanding)\n",
    "    # 3. COCO (objects in context)\n",
    "    # 4. FFHQ (high-quality faces)\n",
    "\n",
    "    # For testing, you can use CIFAR-10 (with upscaling) or a small custom dataset\n",
    "    dataset = datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    student.train()\n",
    "    teacher.eval()  # Teacher is always in eval mode\n",
    "    predictor.train()\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        for batch_idx, (images, _) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Forward pass and loss calculation\n",
    "            loss = trainer.loss(images)\n",
    "\n",
    "            # Normalize loss for gradient accumulation\n",
    "            loss = loss / config[\"accumulation_steps\"]\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize only after accumulation steps\n",
    "            if (batch_idx + 1) % config[\"accumulation_steps\"] == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Update teacher with EMA\n",
    "                trainer.update_teacher()\n",
    "\n",
    "            # Logging\n",
    "            if global_step % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch}, Step: {global_step}, Loss: {loss.item() * config['accumulation_steps']}\"\n",
    "                )\n",
    "\n",
    "            if global_step % 200 == 0:  # every few steps\n",
    "                with torch.no_grad():\n",
    "                    t = torch.randint(0, trainer.n_steps, (images.shape[0],), device=device)\n",
    "                    x_t, _ = trainer.forward_diffusion(images, t)\n",
    "                    teacher_repr = trainer.teacher(images)[:, 0]\n",
    "                    student_repr = trainer.student(x_t)[:, 0]\n",
    "                    sim = F.cosine_similarity(teacher_repr, student_repr, dim=-1).mean()\n",
    "                print(f\"Step {global_step}: Cosine Sim = {sim.item():.4f}\")\n",
    "\n",
    "            # Save checkpoint\n",
    "            if global_step % config[\"save_interval\"] == 0:\n",
    "                checkpoint = {\n",
    "                    \"global_step\": global_step,\n",
    "                    \"student_state_dict\": student.state_dict(),\n",
    "                    \"teacher_state_dict\": teacher.state_dict(),\n",
    "                    \"predictor_state_dict\": predictor.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                }\n",
    "                torch.save(checkpoint, f\"checkpoints/checkpoint_{global_step}.pth\")\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b2c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
