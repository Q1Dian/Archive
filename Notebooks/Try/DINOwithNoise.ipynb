{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e10341b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "import os\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f2c56df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4afbd4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af48d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT_S_16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            \"vit_small_patch16_224\", pretrained=False, num_classes=0, global_pool=\"\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da97b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DINO like structure with noise instead of masking for x1 and x2, the goal is to have the representation of the clean iamge to be the same as itself with additional guassian noise added on it\n",
    "def gather(consts: torch.Tensor, t: torch.Tensor):\n",
    "    \"\"\"Gather constants for specific timesteps t.\"\"\"\n",
    "    c = consts.gather(-1, t)\n",
    "    return c.reshape(-1, 1, 1, 1)\n",
    "\n",
    "\n",
    "class DINODiffusionTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        student: nn.Module,\n",
    "        teacher: nn.Module,\n",
    "        predictor: nn.Module,\n",
    "        device: torch.device,\n",
    "        n_steps=1000,\n",
    "        beta_start=1e-4,\n",
    "        beta_end=0.02,\n",
    "        momentum=0.996,\n",
    "    ):\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "        self.predictor = predictor\n",
    "        self.device = device\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # Initialize teacher with student weights\n",
    "        self.teacher.load_state_dict(self.student.state_dict())\n",
    "        # Freeze teacher\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Diffusion parameters\n",
    "        self.n_steps = n_steps\n",
    "        self.betas = torch.linspace(beta_start, beta_end, n_steps).to(device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    def update_teacher(self):\n",
    "        \"\"\"EMA update for teacher network\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for param_s, param_t in zip(\n",
    "                self.student.parameters(), self.teacher.parameters()\n",
    "            ):\n",
    "                param_t.data.mul_(self.momentum).add_(\n",
    "                    param_s.data * (1 - self.momentum)\n",
    "                )\n",
    "\n",
    "    def forward_diffusion(self, x_0: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Sample from q(x_t | x_0)\"\"\"\n",
    "        sqrt_alpha_bar = gather(torch.sqrt(self.alpha_bars), t)\n",
    "        sqrt_one_minus_alpha_bar = gather(torch.sqrt(1.0 - self.alpha_bars), t)\n",
    "\n",
    "        noise = torch.randn_like(x_0)\n",
    "        return sqrt_alpha_bar * x_0 + sqrt_one_minus_alpha_bar * noise, noise\n",
    "\n",
    "    def loss(self, x_0: torch.Tensor):\n",
    "        \"\"\"DINO-Diffusion loss function\"\"\"\n",
    "        # Sample random timesteps\n",
    "        t = torch.randint(0, self.n_steps, (x_0.shape[0],), device=self.device)\n",
    "\n",
    "        # Create noisy versions\n",
    "        x_t, noise = self.forward_diffusion(x_0, t)\n",
    "\n",
    "        # Get representations\n",
    "        with torch.no_grad():\n",
    "            # Teacher processes clean image\n",
    "            teacher_repr = self.teacher(x_0)\n",
    "            # Use CLS token as global representation\n",
    "            teacher_global = teacher_repr[:, 0]  # [B, D]\n",
    "            # Center and sharpen teacher output (DINO-specific)\n",
    "            teacher_global = F.softmax(teacher_global / 0.07, dim=-1)\n",
    "\n",
    "        # Student processes noisy image\n",
    "        student_repr = self.student(x_t)\n",
    "        student_global = student_repr[:, 0]  # [B, D]\n",
    "        # Predictor transforms student output\n",
    "        predicted_repr = self.predictor(student_global)\n",
    "\n",
    "        # DINO loss: cross-entropy between predicted and teacher distributions\n",
    "        loss = F.cross_entropy(predicted_repr, teacher_global)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fcd7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(im, path):\n",
    "    Image.fromarray(im).save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e79a2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Student params: 21665664\n",
      "Teacher params: 21665664\n",
      "Predictor params: 1181568\n",
      "Epoch: 0, Step: 0, Loss: 6.009171009063721\n",
      "Epoch: 0, Step: 100, Loss: 1.792388916015625\n",
      "Epoch: 0, Step: 200, Loss: 1.4926644563674927\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 95\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         images = \u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m         \u001b[38;5;66;03m# Forward pass and loss calculation\u001b[39;00m\n\u001b[32m     98\u001b[39m         loss = trainer.loss(images)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Main\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"samples\", exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    \"n_steps\": 1000,\n",
    "    \"student_momentum\": 0.996,\n",
    "    \"batch_size\": 128,  # Increased from 1 for better training\n",
    "    \"epochs\": 100,  # Increased for meaningful training\n",
    "    \"save_interval\": 1000,  # Save every 1000 steps\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"accumulation_steps\": 1,  # Gradient accumulation steps\n",
    "    \"image_size\": 224,  # ViT-S/16 expects 224x224 images\n",
    "}\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize models\n",
    "student = VIT_S_16().to(device)\n",
    "teacher = VIT_S_16().to(device)\n",
    "embed_dim = student.backbone.num_features\n",
    "predictor = nn.Sequential(\n",
    "    nn.Linear(embed_dim, 4 * embed_dim),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(4 * embed_dim, embed_dim),\n",
    ").to(device)\n",
    "\n",
    "# Print parameter count\n",
    "print(f\"Student params: {sum(p.numel() for p in student.parameters())}\")\n",
    "print(f\"Teacher params: {sum(p.numel() for p in teacher.parameters())}\")\n",
    "print(f\"Predictor params: {sum(p.numel() for p in predictor.parameters())}\")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = DINODiffusionTrainer(\n",
    "    student=student,\n",
    "    teacher=teacher,\n",
    "    predictor=predictor,\n",
    "    device=device,\n",
    "    n_steps=config[\"n_steps\"],\n",
    ")\n",
    "\n",
    "# Setup optimizer (only optimize student and predictor)\n",
    "optimizer = optim.AdamW(\n",
    "    list(student.parameters()) + list(predictor.parameters()),\n",
    "    lr=config[\"learning_rate\"],\n",
    "    weight_decay=0.05,\n",
    ")\n",
    "\n",
    "# Setup dataset and dataloader\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Recommended datasets for training:\n",
    "# 1. ImageNet (large-scale, best results)\n",
    "# 2. Places365 (scene understanding)\n",
    "# 3. COCO (objects in context)\n",
    "# 4. FFHQ (high-quality faces)\n",
    "\n",
    "# For testing, you can use CIFAR-10 (with upscaling) or a small custom dataset\n",
    "dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "student.train()\n",
    "teacher.eval()  # Teacher is always in eval mode\n",
    "predictor.train()\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    for batch_idx, (images, _) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Forward pass and loss calculation\n",
    "        loss = trainer.loss(images)\n",
    "\n",
    "        # Normalize loss for gradient accumulation\n",
    "        loss = loss / config[\"accumulation_steps\"]\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize only after accumulation steps\n",
    "        if (batch_idx + 1) % config[\"accumulation_steps\"] == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update teacher with EMA\n",
    "            trainer.update_teacher()\n",
    "\n",
    "        # Logging\n",
    "        if global_step % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch}, Step: {global_step}, Loss: {loss.item() * config['accumulation_steps']}\"\n",
    "            )\n",
    "\n",
    "        # Save checkpoint\n",
    "        if global_step % config[\"save_interval\"] == 0:\n",
    "            checkpoint = {\n",
    "                \"global_step\": global_step,\n",
    "                \"student_state_dict\": student.state_dict(),\n",
    "                \"teacher_state_dict\": teacher.state_dict(),\n",
    "                \"predictor_state_dict\": predictor.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(checkpoint, f\"checkpoints/checkpoint_{global_step}.pth\")\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b2c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
